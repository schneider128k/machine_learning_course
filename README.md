# repo for CAP 5610 Machine Learning

### Common ML problems, overview of (a) supervised, (b) unsupervised, and (c) reinforcement learning

  - [1 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_slides.pdf)

### ML terminology, linear regression, training & loss, gradient descent, stochastic gradient descent

  - [2 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_slides.pdf)

### Linear regression using the normal equation - numpy implementation 

  To understand the mathematics underlying the normal equation, read the following materials:

  - [Chapter 2 Linear Algebra](https://www.deeplearningbook.org/contents/linear_algebra.html)
  
  - [Chapter 4 Numerical Computation, Section 4.3 Gradient-Based Optimization](https://www.deeplearningbook.org/contents/numerical.html) 
  
  - [Chapter 5 Machine Learning Basics, Subsection 5.1.4 Example: Linear Regression](https://www.deeplearningbook.org/contents/ml.html)
  
  - [Additional materials: proof of convexity of MSE and computation of gradient of MSE](https://github.com/schneider128k/machine_learning_course/blob/master/slides/linear_regression.pdf)
  
  - [Colab notebook for solving linear regression using normal equation](https://colab.research.google.com/drive/1J7yct9aGfhtfXw8n00Mq4R-xldSSM1WY)

### Effect of learning rate on gradient descent

  - [Colab notebook for experimenting with different learning rates](https://colab.research.google.com/drive/1eECClMU1r-Y9hzPnRw89__jC3nw3C-zD)
   
### Linear regression using gradient descent - numpy implementation

  - [Colab notebook for solving linear regression using gradient descent](https://colab.research.google.com/drive/1qBxfTPoNcSFvpwu1NDl1V6cHEqL3aQl-)

### Overview of TensorFlow and Keras

  - [3 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/3_slides.pdf)

  - [Anatomy of a neural network](https://github.com/schneider128k/machine_learning_course/blob/master/slides/anatomy_of_neural_network.md)

  - [4 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/4_slides.pdf)

### Keras examples

  - [Colab notebook for solving linear regression for artificial data set](https://colab.research.google.com/drive/1pOFL4Qm6WOn2Nxxy6_HteEqQMxStTwzs)
  
  - [Colab notebook for loading and exploring the MNIST digits data set](https://colab.research.google.com/drive/1HDZB0sEjhd0sdTFNCmJXvB8hYnE9KBM7)
  
  - [Colab notebook for classifying MNIST digits with dense layers and analyzing model performance](https://colab.research.google.com/drive/144nj1SRtSjpIcKZgH6-GPdA9bWkg68nh)
  
  - [Colab notebook for classifying MNIST fashion items with dense layers and analyzing model performance](https://colab.research.google.com/drive/1TTO7P5GTmsHhIt_YGqZYyw4KGBCnjqyW)

  - [Colab notebook for displaying CIFAR10 data set](https://colab.research.google.com/drive/1LZZviWOzvchcXRdZi2IBx3KOpQOzLalf)

### Generalization, overfitting, splitting data in train & test sets

  - [5 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/5_slides.pdf)
  
### Validation

  - [6 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/6_slides.pdf)

### Logistic regression, gradients for squared error and binary cross-entropy loss functions

  - [Logistic regression notes](https://github.com/schneider128k/machine_learning_course/blob/master/slides/logistic_regression.pdf)

### Softmax, categorical cross entropy loss, gradients

  - [Softmax, categorical cross entropy](https://github.com/schneider128k/machine_learning_course/blob/master/slides/softmax.pdf)
  
  - [Colab notebook for verifying formulas for partial derivatives with symbolic differentiation](https://colab.research.google.com/drive/1G8u6w3FFhZyb0nWfparVvn77DSjHyxEW)

### Sequential neural networks with dense layers

  - [Notes on forward propagation, backpropagation algorithm for computing partial derivatives wrt weights and biases](https://github.com/schneider128k/machine_learning_course/blob/master/slides/neural_networks.pdf)
  
  - [Code for creating sequential neural networks with dense layers and training them with backprop and mini-batch SGD](https://github.com/schneider128k/machine_learning_course/blob/master/code/neural_network.py); currently, code is limited to (1) mean squared error loss and (2) sigmoid activations.

### Deep learning for computer vision (convolutional neural networks)

  - [Colab notebook for classifying MNIST digits with convolutional layers and analyzing model performance](https://colab.research.google.com/drive/1HA-PoWkxgYa37McvUrA_n609jkKT5F7m)
  
  - [Colab notebook for classifying MNIST fashion items with convolutional layers and anlyzing model performance](https://colab.research.google.com/drive/1eI47U0vW_5ok6rVvNuenXS2EkTbqxcCV)

### Deep learning for text and sequences (RNN, LSTM)

### Generative deep learning (VAE, GAN)

---

Tools, additional materials

- [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/), Michael Nielsen

- Deep learning with Python, Francois Chollet, Manning 

- Deep learning and the game of Go, Max Pumperla and Kevin Ferguson, Mannin

- [Deep learning book](https://www.deeplearningbook.org/), Ian Goodfellow and Yoshua Bengio and Aaron Courville, MIT Press

- [Google's machine learning materials](https://developers.google.com/machine-learning/crash-course/)

  Nice visualizations of the neural networks.

- [Google's colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb)

- [TensorFlow](https://www.tensorflow.org/)

- [Keras](https://keras.io/)

- [Anaconda Python](https://www.anaconda.com/)

- [Getting started with conda](https://conda.io/docs/user-guide/overview.html)

- [PyCharm](https://www.jetbrains.com/pycharm/)

- [Git](https://git-scm.com/)

- [Git documentation](https://git-scm.com/doc)


