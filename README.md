# machine_learning_course

CAP 5610 Machine Learning

Machine learning course at UCF

- **Common ML problems, overview of (a) supervised, (b) unsupervised, and (c) reinforcement learning**

  [1 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/1_slides.pdf)

- **ML terminology, linear regression, training & loss, gradient descent, stochastic gradient descent** 

  [2 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/2_slides.pdf)

- **Linear regression using the normal equation - numpy implementation**

  To understand the mathematics underlying the normal equation, read the following materials:

  [Chapter 2 Linear Algebra](https://www.deeplearningbook.org/contents/linear_algebra.html)
  
  [Chapter 4 Numerical Computation, Section 4.3 Gradient-Based Optimization](https://www.deeplearningbook.org/contents/numerical.html) 
  
  [Chapter 5 Machine Learning Basics, Subsection 5.1.4 Example: Linear Regression](https://www.deeplearningbook.org/contents/ml.html)
  
  [Additional materials: proof of convexity of MSE and computation of gradient of MSE](https://github.com/schneider128k/machine_learning_course/blob/master/slides/linear_regression.pdf)

  [Colab notebook for solving linear regression using normal equation](https://colab.research.google.com/drive/1J7yct9aGfhtfXw8n00Mq4R-xldSSM1WY)

- **Effect of learning rate on gradient descent**

  [Colab notebook for experimenting with different learning rates](https://colab.research.google.com/drive/1eECClMU1r-Y9hzPnRw89__jC3nw3C-zD)
   
- **Linear regression using gradient descent - numpy implementation**

  [Colab notebook for solving linear regression using gradient descent](https://colab.research.google.com/drive/1qBxfTPoNcSFvpwu1NDl1V6cHEqL3aQl-)

- **Overview of TensorFlow and Keras**

  [3 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/3_slides.pdf)

- **Linear regression - Keras implementation**

  [Colab notebook for solving linear regression](https://colab.research.google.com/drive/1pOFL4Qm6WOn2Nxxy6_HteEqQMxStTwzs)

- **Generalization, overfitting, splitting data in train & test sets**

  [4 Slides](https://github.com/schneider128k/machine_learning_course/blob/master/4_slides.pdf)

---

Tools, additional materials

- [Deep learning book](https://www.deeplearningbook.org/)

- [Anaconda Python](https://www.anaconda.com/)

- [Google's colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb)

- [Google's machine learning materials](https://developers.google.com/machine-learning/crash-course/)

- [TensorFlow](https://www.tensorflow.org/)

- [Getting started with conda](https://conda.io/docs/user-guide/overview.html)

- [PyCharm](https://www.jetbrains.com/pycharm/)

- [Git](https://git-scm.com/)

- [Git documentation](https://git-scm.com/doc)


